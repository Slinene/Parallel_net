import keras
from keras.models import Sequential, load_model
from keras.layers.core import  Reshape
from keras.layers import * #Dense, LSTM, Dropout, GRU, Conv2D, MaxPooling2D, Flatten, merge, Merge, UpSampling2D, \
   # ZeroPadding2D, GlobalAveragePooling2D,BatchNormalization
from keras.callbacks import EarlyStopping, TensorBoard,ReduceLROnPlateau
from keras.optimizers import RMSprop, Adam, SGD,Nadam, Adamax
from keras.utils import plot_model
import time
from keras import backend as K
#from dataPrepocessing import *
from keras.applications.inception_v3 import *
import tensorflow as tf
#from new_parameterTuning import *


def flip_axis(x, axis):
    x = np.asarray(x).swapaxes(axis,0)
    x = x[::-1, ...]
    x = x.swapaxes(0, axis)
    return x


def data_aug(x_train,y_train):
    #needs to be implemented

    return x_train,y_train

def conv2d_bn(x,
              filters,
              num_row,
              num_col,
              padding='same',
              strides=(1, 1),
              name=None):
    """Utility function to apply conv + BN.

    # Arguments
        x: input tensor.
        filters: filters in `Conv2D`.
        num_row: height of the convolution kernel.
        num_col: width of the convolution kernel.
        padding: padding mode in `Conv2D`.
        strides: strides in `Conv2D`.
        name: name of the ops; will become `name + '_conv'`
            for the convolution and `name + '_bn'` for the
            batch norm layer.

    # Returns
        Output tensor after applying `Conv2D` and `BatchNormalization`.
    """
    if name is not None:
        bn_name = name + '_bn'
        conv_name = name + '_conv'
    else:
        bn_name = None
        conv_name = None
    if K.image_data_format() == 'channels_first':
        bn_axis = 1
    else:
        bn_axis = 3
    x = Conv2D(
        filters, (num_row, num_col),
        strides=strides,
        padding=padding,
        use_bias=False,
        name=conv_name)(x)
    x = BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)
    x = Activation('relu', name=name)(x)
    return x


def my_loss_abs_error_pct(y_true, y_pred):
    mask = K.sign(y_true)
    abs_error = K.abs(y_true-y_pred) * mask
    # print(abs_error)
    # print(K.int_shape(abs_error))
    # aa = K.constant(7.99, shape=K.int_shape(abs_error))
    abs_error_largerthan8db = K.sum(K.sign(abs_error - 7.99) + 1, axis=None)/2
    return abs_error_largerthan8db / K.sum(mask, axis=None)




def InceptionV3_2layer_100(img_input,input_para, num_outputs,activation, nfilter):
# def myModel_trail():
    if K.image_data_format() == 'channels_first':
        channel_axis = 1
    else:
        channel_axis = 3

    # model = Sequential()
#    img_input = Input(shape=(100, 100, 9))
    x = conv2d_bn(img_input, nfilter[0], 3, 3, strides=(1, 1), padding='valid')
    x=  BatchNormalization(momentum=0.99, epsilon=0.001)(x)
    
    x = conv2d_bn(img_input, nfilter[0], 3, 3, strides=(1, 1), padding='same')
    x=  BatchNormalization(momentum=0.99, epsilon=0.001)(x)
    
    x = conv2d_bn(img_input, nfilter[0], 5, 5, strides=(1, 1), padding='same')
    x=  BatchNormalization(momentum=0.99, epsilon=0.001)(x)
    
    x = conv2d_bn(img_input, nfilter[0], 3, 3, strides=(1, 1), padding='same')
    x=  BatchNormalization(momentum=0.99, epsilon=0.001)(x)
    
    x = conv2d_bn(img_input, nfilter[0], 3, 3, strides=(1, 1), padding='same')
    x=  BatchNormalization(momentum=0.99, epsilon=0.001)(x)
    
    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)
    # x = Dropout(0.5)(x)
    # x = conv2d_bn(x, 8, 3, 3)
    x = conv2d_bn(x, nfilter[1], 5, 5)
    x=  BatchNormalization(momentum=0.99, epsilon=0.001)(x)
    x = conv2d_bn(x, nfilter[1], 5, 5)
    x=  BatchNormalization(momentum=0.99, epsilon=0.001)(x)
    
    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)

    # mixed 0, 1, 2: 35 x 35 x 256

# ##-------------------------------------------------
    branch1x1 = conv2d_bn(x, 8, 1, 1)
#
    branch5x5 = conv2d_bn(x, 12, 1, 1)
    branch5x5 = conv2d_bn(branch5x5, 24, 3, 3)

    branch3x3dbl = conv2d_bn(x, 8, 1, 1)
    branch3x3dbl = conv2d_bn(branch3x3dbl, 16, 3, 3)
    branch3x3dbl = conv2d_bn(branch3x3dbl, 16, 3, 3)

    branch_pool = AveragePooling2D((2, 2), strides=(1, 1), padding='same')(x)
    branch_pool = conv2d_bn(branch_pool, 16, 1, 1)
    x = layers.concatenate([branch1x1, branch5x5, branch3x3dbl, branch_pool], axis=channel_axis, name='mixed0_c')
#

    branch1x1 = conv2d_bn(x, 8, 1, 1)

    branch5x5 = conv2d_bn(x, 12, 1, 1)
    branch5x5 = conv2d_bn(branch5x5, 24, 3, 3)

    branch3x3dbl = conv2d_bn(x, 8, 1, 1)
    branch3x3dbl = conv2d_bn(branch3x3dbl, 16, 3, 3)
    branch3x3dbl = conv2d_bn(branch3x3dbl, 16, 3, 3)

    branch_pool = AveragePooling2D((2, 2), strides=(1, 1), padding='same')(x)
    branch_pool = conv2d_bn(branch_pool, 8, 1, 1)
    x = layers.concatenate([branch1x1, branch5x5, branch3x3dbl, branch_pool], axis=channel_axis, name='mixed1_c')

    branch1x1 = conv2d_bn(x, 8, 1, 1)
    branch5x5 = conv2d_bn(x, 12, 1, 1)
    branch5x5 = conv2d_bn(branch5x5, 24, 3, 3)

    branch3x3dbl = conv2d_bn(x, 8, 1, 1)
    branch3x3dbl = conv2d_bn(branch3x3dbl, 16, 3, 3)
    branch3x3dbl = conv2d_bn(branch3x3dbl, 16, 3, 3)

    branch_pool = AveragePooling2D((2, 2), strides=(1, 1), padding='same')(x)
    branch_pool = conv2d_bn(branch_pool, 8, 1, 1)
    x = layers.concatenate([branch1x1, branch5x5, branch3x3dbl, branch_pool], axis=channel_axis, name='mixed2_c')
    x = BatchNormalization(momentum=0.99, epsilon=0.001)(x)
##------------------------------------------------------------------------------------------
    #
    branch1x1 = conv2d_bn(x, 8, 1, 1)
    
    branch5x5 = conv2d_bn(x, 8, 1, 1)
    branch5x5 = conv2d_bn(branch5x5, 8, 3, 3)
    
    branch3x3dbl = conv2d_bn(x, 8, 1, 1)
    branch3x3dbl = conv2d_bn(branch3x3dbl, 8, 3, 3)
    branch3x3dbl = conv2d_bn(branch3x3dbl, 8, 3, 3)
    
    branch_pool = AveragePooling2D((2, 2), strides=(1, 1), padding='same')(x)
    branch_pool = conv2d_bn(branch_pool, 8, 1, 1)
    x = layers.concatenate([branch1x1, branch5x5, branch3x3dbl, branch_pool], axis=channel_axis, name='mixed3_c')
    x=  BatchNormalization(momentum=0.99, epsilon=0.001)(x)

#    for i in range(2):
#         branch1x1 = conv2d_bn(x, 16, 1, 1)
#    
#         branch3x3 = conv2d_bn(x, 8, 1, 1)
#         branch3x3_1 = conv2d_bn(branch3x3, 8, 1, 3)
#         branch3x3_2 = conv2d_bn(branch3x3, 8, 3, 1)
#         branch3x3 = layers.concatenate(
#             [branch3x3_1, branch3x3_2], axis=channel_axis, name='mixed9_' + str(i))
#    
#         branch3x3dbl = conv2d_bn(x, 8, 1, 1)
#         branch3x3dbl = conv2d_bn(branch3x3dbl, 8, 3, 3)
#         branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 8, 1, 3)
#         branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 8, 3, 1)
#         branch3x3dbl = layers.concatenate(
#             [branch3x3dbl_1, branch3x3dbl_2], axis=channel_axis)
#         
#         branch_pool = AveragePooling2D(
#             (3, 3), strides=(1, 1), padding='same')(x)
#         branch_pool = conv2d_bn(branch_pool, 8, 1, 1)
#         x = layers.concatenate(
#             [branch1x1, branch3x3, branch3x3dbl, branch_pool],
#             axis=channel_axis,
#             name='mixed' + str(9 + i))
#         x=  BatchNormalization(momentum=0.99, epsilon=0.001)(x)

#     x = GlobalMaxPooling2D()(x)
#     if include_top:
#         # Classification block
#         x = GlobalAveragePooling2D(name='avg_pool')(x)
#         x = Dense(classes, activation='softmax', name='predictions')(x)
#     else:
#         if pooling == 'avg':
#             x = GlobalAveragePooling2D()(x)
#         elif pooling == 'max':
#             x = GlobalMaxPooling2D()(x)

# #----------------------------------
    branch1x1 = conv2d_bn(x, 16, 3, 3)

    branch5x5 = conv2d_bn(x, 12, 3, 3)
    branch5x5 = conv2d_bn(branch5x5, 64, 3, 3)

    branch3x3dbl = conv2d_bn(x, 16, 5, 5)
    branch3x3dbl = conv2d_bn(branch3x3dbl, 24, 3, 3)
    branch3x3dbl = conv2d_bn(branch3x3dbl, 24, 3, 3)

    branch_pool = AveragePooling2D((2, 2), strides=(1, 1), padding='same')(x)
    branch_pool = conv2d_bn(branch_pool, 16, 1, 1)
    x = layers.concatenate([branch1x1, branch5x5, branch3x3dbl, branch_pool], axis=channel_axis, name='mixed4_c')
    x=  BatchNormalization(momentum=0.99, epsilon=0.001)(x)
#     # branch1x1 = conv2d_bn(x, 16, 1, 1)
    #
    # branch5x5 = conv2d_bn(x, 12, 1, 1)
    # branch5x5 = conv2d_bn(branch5x5, 16, 5, 5)
    #
    # branch3x3dbl = conv2d_bn(x, 16, 1, 1)
    # branch3x3dbl = conv2d_bn(branch3x3dbl, 24, 3, 3)
    # branch3x3dbl = conv2d_bn(branch3x3dbl, 24, 3, 3)
    #
    # branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)
    # branch_pool = conv2d_bn(branch_pool, 16, 1, 1)
    # x = layers.concatenate([branch1x1, branch5x5, branch3x3dbl, branch_pool], axis=channel_axis, name='mixed1')

#---------------------------
    # x = UpSampling2D(size=(2, 2))(x)
    # x = conv2d_bn(x, nfilter[2], 3, 3)
    x=GlobalAveragePooling2D(data_format=K.image_data_format())(x)
#---------------------------
#    flatten1 = Flatten()(x)
    flatten1=concatenate([x,input_para],axis=-1)
    dense = Dense(units=num_outputs/4, kernel_initializer="he_normal",
                      activation="relu")(flatten1)
    x=Reshape((14,14,1))(dense)
    
    
    x = UpSampling2D(size=(2, 2))(x)
    x= conv2d_bn(x, nfilter[2], 3, 3)
    x=  BatchNormalization(momentum=0.99, epsilon=0.001)(x)
    x= conv2d_bn(x, nfilter[2], 3, 3)
    x=  BatchNormalization(momentum=0.99, epsilon=0.001)(x)
    x= conv2d_bn(x, nfilter[2], 3, 3)
    x=  BatchNormalization(momentum=0.99, epsilon=0.001)(x)
    x = UpSampling2D(size=(2, 2))(x)
    # x = ZeroPadding2D(padding=(1, 1))(x)
    x = conv2d_bn(x, nfilter[3], 3, 3)
    x=  BatchNormalization(momentum=0.99, epsilon=0.001)(x)
    x = conv2d_bn(x, nfilter[3], 3, 3)
    x=  BatchNormalization(momentum=0.99, epsilon=0.001)(x)
    x = conv2d_bn(x, nfilter[3], 3, 3)
    x=  BatchNormalization(momentum=0.99, epsilon=0.001)(x)
    x = conv2d_bn(x, nfilter[3], 3, 3)
    x=  BatchNormalization(momentum=0.99, epsilon=0.001)(x)
    x = Conv2D(1, (3, 3), activation='relu', padding='same')(x)
    return(x)
#    model = Model(img_input, x, name='inception_cxy_v1')
#    plot_model(model, show_shapes=True, to_file=os.path.join(modelPath, "model_cxy.png"))
#
#    if optType == 'Nadam':
#        opt = Nadam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)  # [1e-3 1e-4 1e-5 1e-6]
#    elif optType == 'Adam':
#        opt = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0)
#    elif optType == 'Adamax':
#        opt = Adamax(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0)
#    else:
#        print('optType error')
#        return 0
#
#    model.compile(loss=myloss, optimizer=opt,
#                  metrics=[my_loss_abs_error_pct, 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error'])
#    early_stop = EarlyStopping(monitor="val_loss", patience=patience,verbose=0, mode='auto')
#    tensorboard = TensorBoard(log_dir=os.path.join(modelPath, 'log' + str(model_opt_idx) + '.h5'), histogram_freq=0, write_graph=True, write_images=True)
#
#    #
#    model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=os.path.join(modelPath, 'best' + str(model_opt_idx) + '.h5'), monitor='val_loss', verbose=1,save_best_only=True)
#    hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, shuffle=True,
#              callbacks=[early_stop,tensorboard,model_checkpoint])
#    #model.load_weights(filepath=os.path.join(modelPath, 'best' + str(model_opt_idx) + '.h5'))
#
#    #
#    model.save_weights(filepath=os.path.join(modelPath, 'best' + str(model_opt_idx) + '.h5').format(epochs))
#
#
#    early_stop = EarlyStopping(monitor="val_loss", patience=patience)
#    tensorboard = TensorBoard(log_dir=os.path.join(modelPath, 'log' + str(model_opt_idx) + '.h5'), histogram_freq=0, write_graph=True, write_images=True)
#    hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, shuffle=True,
#              callbacks=[early_stop])
#    model.save(filepath=os.path.join(modelPath, 'mode5m' + str(model_opt_idx) + '.h5'), overwrite=True)
#    #model = load_model(os.path.join(modelPath, 'model' + str(model_opt_idx) + '.h5'))
#    return model, hist

